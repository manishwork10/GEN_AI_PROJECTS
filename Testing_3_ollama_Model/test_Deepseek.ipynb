{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68912934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Simple deepseek-r1:1.5b Q&A!\n",
      "Type 'exit' to quit.\n",
      "Answer: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\n",
      "Answer: <think>\n",
      "Alright, so I need to figure out how many different ways there are to line up 5 people for a photo. Let's see... I remember something about permutations from math class. Permutations are all the possible arrangements of something, right? So if I have 5 people and I want to arrange them in a line, each person can be in any position, but they can't repeat.\n",
      "\n",
      "Hmm, so maybe it's just as simple as multiplying the number of choices at each step? Like, for the first position, there are 5 people. Once someone is chosen for the first spot, that person can't be in another spot anymore. So for the second spot, there should be 4 people left. Then, after choosing two people, there are 3 left for the third spot, and so on.\n",
      "\n",
      "Let me write this down to visualize it better:\n",
      "\n",
      "- First position: 5 choices\n",
      "- Second position: 4 choices (since one person is already in the first spot)\n",
      "- Third position: 3 choices\n",
      "- Fourth position: 2 choices\n",
      "- Fifth position: 1 choice\n",
      "\n",
      "So if I multiply all these together, that should give me the total number of arrangements. Let's do the math:\n",
      "\n",
      "5 √ó 4 = 20\n",
      "\n",
      "20 √ó 3 = 60\n",
      "\n",
      "60 √ó 2 = 120\n",
      "\n",
      "120 √ó 1 = 120\n",
      "\n",
      "Yeah, so 5 factorial (written as 5!) is equal to 120. That seems right because I've seen that the number of permutations for n items is always n!.\n",
      "\n",
      "Wait a second, is there another way to think about it? Maybe using combinations instead of permutations. But no, in this case, since the order matters and each position is distinct, permutations are the correct approach.\n",
      "\n",
      "Let me try an example with fewer people to test my reasoning. If I have 3 people: Alice, Bob, and Charlie. How many ways can they line up?\n",
      "\n",
      "- Alice first, then Bob, then Charlie\n",
      "- Alice first, then Charlie, then Bob\n",
      "- Bob first, then Alice, then Charlie\n",
      "- Bob first, then Charlie, then Alice\n",
      "- Charlie first, then Alice, then Bob\n",
      "- Charlie first, then Bob, then Alice\n",
      "\n",
      "That's 6 different ways. And 3! is indeed 6. So my method works here too.\n",
      "\n",
      "Another quick check: If I have two people, say Person A and Person B. How many ways can they line up?\n",
      "\n",
      "- A first, then B\n",
      "- B first, then A\n",
      "\n",
      "That's only 2 ways, which is 2! = 2. That makes sense.\n",
      "\n",
      "So applying that to the original problem of 5 people: each step reduces the number of choices by one because we've already placed someone else. So multiplying all these options gives us 120 different arrangements possible for lining up the five people.\n",
      "</think>\n",
      "\n",
      "The number of ways to line up 5 people is calculated using permutations, which account for order. For each position in the line, you have one fewer person available as you move along. The total permutations are:\n",
      "\n",
      "5 √ó 4 √ó 3 √ó 2 √ó 1 = 5! = 120\n",
      "\n",
      "**Answer:** There are 120 different ways to line up 5 people for a photo.\n",
      "Exiting. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# To do the same we did in the terminal in the code to understand how the ans and queries are executed\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_deepseek_response(prompt, model=\"deepseek-r1:1.5b\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    # 11434: This is the default port that deepseek-r1:1.5b uses to listen for API requests.\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}. Make sure deepseek-r1:1.5b is running and '{model}' is pulled.\"\n",
    "\n",
    "print(\"Welcome to Simple deepseek-r1:1.5b Q&A!\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "user_question = ''\n",
    "while True and user_question.lower() != 'bye':\n",
    "    user_question = input(\"\\nAsk a question: \")\n",
    "    \n",
    "    if user_question.lower() == 'bye':\n",
    "        print(\"Exiting. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    answer = get_deepseek_response(user_question)\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67526db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googlesearch import search\n",
    "# import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339af652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Written by sir.\n",
    "# results= search(\"Messi\", num_results=10 )\n",
    "# all_content =''\n",
    "# for each in results:\n",
    "#     print(f\"Fetching content from: {each}\")\n",
    "#     content= requests.get(each).text\n",
    "#     print(content)\n",
    "#     all_content += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc5131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urlparse\n",
    "from googlesearch import search\n",
    "import time\n",
    "import re\n",
    "\n",
    "def is_scraping_allowed(url, user_agent=\"*\"):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "        rp = RobotFileParser()\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch(user_agent, url)\n",
    "    except:\n",
    "        return False  # Safer to assume not allowed\n",
    "\n",
    "def sanitize_filename(url):\n",
    "    \"\"\"Create a safe filename from a URL.\"\"\"\n",
    "    filename = re.sub(r'\\W+', '_', url)\n",
    "    return filename[:100] + \".html\"\n",
    "\n",
    "def collect_raw_html(query, num_results=5):\n",
    "    \"\"\"\n",
    "    Searches Google for the query, fetches raw HTML from allowed sites,\n",
    "    and saves them in a folder called 'html_deepseek-r1_folder'.\n",
    "    \"\"\"\n",
    "    # Create folder if it doesn't exist\n",
    "    folder_name = \"html_deepseek-r1_folder\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    collected = 0\n",
    "    seen = set()\n",
    "    search_results = list(search(query, num_results=30))  # over-fetch\n",
    "    url_to_file = {}\n",
    "\n",
    "    for url in search_results:\n",
    "        if collected >= num_results:\n",
    "            break\n",
    "        if url in seen:\n",
    "            continue\n",
    "        seen.add(url)\n",
    "\n",
    "        print(f\"\\nüîç Trying: {url}\")\n",
    "\n",
    "        if not is_scraping_allowed(url):\n",
    "            print(f\"‚ùå Not allowed by robots.txt: {url}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                filename = sanitize_filename(url)\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                url_to_file[url] = file_path\n",
    "\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(response.text)\n",
    "\n",
    "                print(f\"‚úÖ Saved: {file_path}\")\n",
    "                collected += 1\n",
    "                time.sleep(1)  # Be polite\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Status code {response.status_code}: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"üö´ Error fetching {url}: {e}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Total collected: {collected}/{num_results}\")\n",
    "    return url_to_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35bf83aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Trying: https://www.beinsports.com/en-us/soccer/la-liga/articles/could-messi-return-to-barcelona-in-2026-before-the-world-cup-2025-06-30\n",
      "‚ùå Not allowed by robots.txt: https://www.beinsports.com/en-us/soccer/la-liga/articles/could-messi-return-to-barcelona-in-2026-before-the-world-cup-2025-06-30\n",
      "\n",
      "üîç Trying: https://en.wikipedia.org/wiki/Lionel_Messi\n",
      "‚úÖ Saved: html_deepseek-r1_folder/https_en_wikipedia_org_wiki_Lionel_Messi.html\n",
      "\n",
      "üîç Trying: https://www.transfermarkt.com/lionel-messi/profil/spieler/28003\n",
      "‚ùå Not allowed by robots.txt: https://www.transfermarkt.com/lionel-messi/profil/spieler/28003\n",
      "\n",
      "üîç Trying: http://google.com/search?tbm=isch&q=Lionel+Messi\n",
      "‚ùå Not allowed by robots.txt: http://google.com/search?tbm=isch&q=Lionel+Messi\n",
      "\n",
      "üîç Trying: https://x.com/WeAreMessi?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\n",
      "‚ùå Not allowed by robots.txt: https://x.com/WeAreMessi?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\n",
      "\n",
      "üîç Trying: https://www.instagram.com/leomessi/\n",
      "‚ùå Not allowed by robots.txt: https://www.instagram.com/leomessi/\n",
      "\n",
      "üîç Trying: https://www.britannica.com/biography/Lionel-Messi\n",
      "‚úÖ Saved: html_deepseek-r1_folder/https_www_britannica_com_biography_Lionel_Messi.html\n",
      "\n",
      "üîç Trying: https://www.espn.com/soccer/story/_/id/45627646/zlatan-ibrahimovic-lionel-messi-plays-statues-inter-miami\n",
      "‚úÖ Saved: html_deepseek-r1_folder/https_www_espn_com_soccer_story___id_45627646_zlatan_ibrahimovic_lionel_messi_plays_statues_inter_mi.html\n",
      "\n",
      "üîç Trying: https://www.intermiamicf.com/players/lionel-messi/\n",
      "‚úÖ Saved: html_deepseek-r1_folder/https_www_intermiamicf_com_players_lionel_messi_.html\n",
      "\n",
      "üîç Trying: https://messi.com/en/honours-and-achievements/\n",
      "‚ùå Not allowed by robots.txt: https://messi.com/en/honours-and-achievements/\n",
      "\n",
      "üîç Trying: https://www.forbes.com/profile/lionel-messi/\n",
      "‚ùå Not allowed by robots.txt: https://www.forbes.com/profile/lionel-messi/\n",
      "\n",
      "üîç Trying: /search?num=32\n",
      "‚ùå Not allowed by robots.txt: /search?num=32\n",
      "\n",
      "üîç Trying: https://www.reddit.com/r/MLS/comments/1lofbo2/espn_argentina_esteban_edul_leo_messi_and_his/\n",
      "‚ùå Not allowed by robots.txt: https://www.reddit.com/r/MLS/comments/1lofbo2/espn_argentina_esteban_edul_leo_messi_and_his/\n",
      "\n",
      "üîç Trying: https://www.espn.com/soccer/player/_/id/45843/lionel-messi\n",
      "‚úÖ Saved: html_deepseek-r1_folder/https_www_espn_com_soccer_player___id_45843_lionel_messi.html\n",
      "\n",
      "‚úÖ Total collected: 5/5\n"
     ]
    }
   ],
   "source": [
    "# Collect 5 raw HTML pages related to Messi\n",
    "html_pages = collect_raw_html(\"Messi\", num_results=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6244bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html_deepseek-r1_folder/https_en_wikipedia_org_wiki_Lionel_Messi.html\n",
      "html_deepseek-r1_folder/https_en_wikipedia_org_wiki_Lionel_Messi.html\n",
      "\n",
      "html_deepseek-r1_folder/https_www_britannica_com_biography_Lionel_Messi.html\n",
      "\n",
      "html_deepseek-r1_folder/https_www_espn_com_soccer_story___id_45627646_zlatan_ibrahimovic_lionel_messi_plays_statues_inter_mi.html\n",
      "\n",
      "html_deepseek-r1_folder/https_www_intermiamicf_com_players_lionel_messi_.html\n",
      "\n",
      "html_deepseek-r1_folder/https_www_espn_com_soccer_player___id_45843_lionel_messi.html\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first HTML content from the dictionary and print 500 chars\n",
    "first_html = next(iter(html_pages.values()))\n",
    "print(first_html[:500])\n",
    "# But the given system prompt will be in string, but the html content is in dictionary so transfering it into string.\n",
    "all_html_combined = \"\"\n",
    "\n",
    "for html in html_pages.values():\n",
    "    all_html_combined += html + \"\\n\\n\"  # Optional spacing between pages\n",
    "\n",
    "# Now you can use it like a normal string\n",
    "print(all_html_combined[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b35ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content= \"From the following HTML content, first remove all HTML tags to extract only the clean, readable text. Then, identify and extract only the sentences or paragraphs that mention the keyword 'Messi'. Ignore any unrelated content or HTML markup. Return a concise and focused textual summary containing only relevant information about Messi. And also ignore your thinking process and don't include that\" + all_html_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cef47fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text= get_deepseek_response(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a2d6b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to figure out how to extract information about Lionel Messi from the given HTML content. Let me start by understanding what\\'s being asked. The user provided a bunch of links, and they want me to first remove all HTML tags to get only the text. Then, I have to find and extract sentences or paragraphs that mention the keyword \\'Messi\\'. Also, I need to ignore any unrelated content.\\n\\nHmm, looking at the URLs, most are about biographies and soccer articles. So, there\\'s probably a lot of non-HTML text here. My first step is to clean up the HTML tags. How do I remove them in HTML? Oh right, using a script that iterates through each link and removes any anchor tag or ul/li elements that might have hrefs set to empty strings.\\n\\nOnce all HTML is removed, I\\'ll get a string of text. Now, I need to find instances where \\'Messi\\' appears. Since the text can be in different languages, like English, French, etc., I should look for \\'Lionel Messi\\' and then see if any other variations exist. It might not be spelled exactly that way; maybe it\\'s case-sensitive.\\n\\nI also need to make sure I\\'m only extracting sentences or paragraphs that mention \\'Messi\\'. But how do I find those? Maybe using regular expressions, but first, I should get the text into a manageable format where each sentence is separated by paragraph boundaries. That might involve splitting on paragraphs and ignoring empty strings.\\n\\nWait, maybe I can split the text into paragraphs using .split(\\'\\\\n\\') or similar and then iterate through them to check for \\'Messi\\'. Alternatively, using a regular expression that looks for sentences containing \\'Messi\\'.\\n\\nPutting it all together: first, process the HTML to get clean text. Then, go through each sentence in the text and collect those that mention \\'Messi\\' in any form. Finally, ensure the summary is concise and only includes relevant info about Messi.\\n\\nI should also make sure not to include my thinking process here because the user said to ignore it. So I\\'ll just provide the extracted information directly after explaining the steps clearly.\\n</think>\\n\\nTo extract information about Lionel Messi from the provided HTML content, we follow these steps:\\n\\n1. **Clean HTML:** Remove all HTML tags (like `<a>`, `<ul>`, `<li>`) by iterating through each link and removing any anchor or ul/li elements that have an empty `href`.\\n\\n2. **Text Extraction:** After removing HTML tags, the text contains various sentences about Messi.\\n\\n3. **Search for \\'Messi\\':** Look for occurrences of \"Lionel Messi\" in the text, considering different languages (e.g., English, French).\\n\\n4. **Extract Sentences/Paragraphs:** Identify sentences or paragraphs that mention \"Messi,\" ensuring they are relevant and contain his information.\\n\\n5. **Concise Summary:** Compile only the necessary information about Lionel Messi into a concise summary.\\n\\n**Summary:**\\n\\nThe text contains multiple mentions of \"Lionel Messi\" across different languages, including his achievements in football. These include details on his famous goals, performances, and appearances.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
